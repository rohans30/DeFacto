{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "\n",
    "def extract_text_from_pdf(file_path):\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        text = \"\"\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "pdf_text = extract_text_from_pdf(\"mock_trial.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated tokens: 5215\n"
     ]
    }
   ],
   "source": [
    "def estimate_tokens(text):\n",
    "    return len(text) // 4\n",
    "\n",
    "total_tokens = estimate_tokens(pdf_text)\n",
    "print(f\"Estimated tokens: {total_tokens}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_prompt = f\"\"\"\n",
    "You are a judge presiding over a mock trial. You must adhere to the trial's context and answer questions as the judge.\n",
    "\n",
    "Here is the trial document:\n",
    "{pdf_text} \n",
    "\n",
    "You are ready to proceed. The user will be roleplaying as another character in the mock trial. \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_owner_prompt = f\"\"\"\n",
    "You are pretending to be the car owner in this mock trial. You must adhere to the trial's context and answer questions as the car owner.\n",
    "\n",
    "Here is the trial document:\n",
    "{pdf_text} \n",
    "\n",
    "You are ready to proceed. The user will be roleplaying as another character in the mock trial. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "from autogen import ConversableAgent\n",
    "import os\n",
    "\n",
    "\n",
    "# llm_config = {\"config_list\": [{\"model\": \"gpt-4o-mini\", \"api_key\": os.environ.get(\"OPENAI_API_KEY\"), \"api_rate_limit\": 1.0}]}\n",
    "llm_config = {\"config_list\": [{\"model\": \"llama3.2:latest\", \"api_type\": \"ollama\", \"client_host\": \"http://127.0.0.1:11434\"}]}\n",
    "\n",
    "    \n",
    "# the main entrypoint/supervisor agent\n",
    "judge_agent = ConversableAgent(\"judge agent\", system_message=judge_prompt, llm_config=llm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_owner_agent = ConversableAgent(name=\"car owner agent\", system_message=car_owner_prompt, llm_config=llm_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_proxy = ConversableAgent(\n",
    "    \"human_proxy\",\n",
    "    llm_config=False,  # no LLM used for human proxy\n",
    "    human_input_mode=\"ALWAYS\",  # always ask for human input\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import GroupChat\n",
    "from autogen import GroupChatManager\n",
    "\n",
    "group_chat = GroupChat(\n",
    "    agents=[judge_agent, car_owner_agent, human_proxy],\n",
    "    messages=[],\n",
    "    max_round=6,\n",
    ")\n",
    "\n",
    "\n",
    "group_chat_manager = GroupChatManager(\n",
    "    groupchat=group_chat,\n",
    "    llm_config=llm_config \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mhuman_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "I will be roleplaying as the Deputy DA in this mock trial, is everyone ready?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: car owner agent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mcar owner agent\u001b[0m (to chat_manager):\n",
      "\n",
      "I'll play the role of the defendant's attorney. My client, John Smith, has been accused of embezzling funds from a local charity. I'm prepared to defend him against these serious charges.\n",
      "\n",
      "Before we begin, let me just review the case files and make sure I have all the necessary materials.\n",
      "\n",
      "Also, as the Deputy DA, please go ahead and proceed with opening statements. What would you like to say to the court about the evidence and the case?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: human_proxy\n",
      "\u001b[0m\n",
      "\u001b[33mhuman_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "Why did you steal the car?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: judge agent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mjudge agent\u001b[0m (to chat_manager):\n",
      "\n",
      "Look, I didn't steal the car. I was just driving it because my friend Rick had stolen it and we were trying to figure out what to do with it. I had never been in a car like that before, so I wasn't thinking clearly when the officer pulled me over. It was all a big misunderstanding.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: car owner agent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mcar owner agent\u001b[0m (to chat_manager):\n",
      "\n",
      "I think there's been a misunderstanding from the start. My client is not claiming that you stole the car, but rather that you were trying to return it to its rightful owner. We've presented evidence that suggests your friend Rick was the actual thief, and that your actions in driving the car back to Martinez were a genuine attempt to rectify the situation.\n",
      "\n",
      "The prosecution's expert testified that other fingerprints were found on the keys that couldn't be matched to you or your friend Rick. That raises more questions about who actually stole the car than it answers.\n",
      "\n",
      "I'd like to ask you, did you and your friend Rick have any intention of keeping the car for yourselves? Did you plan on selling it or using it in some way? Or were you simply trying to figure out what to do with it because you knew it didn't belong to either of you?\n",
      "\n",
      "And can you think of any reason why the police would think that you stole the car, when we've presented evidence that suggests otherwise?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: judge agent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "chat_result = human_proxy.initiate_chat(\n",
    "    group_chat_manager,\n",
    "    message=\"I will be roleplaying as the Deputy DA in this mock trial, is everyone ready?\",\n",
    "    summary_method=\"reflection_with_llm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='Understood. I am prepared to answer questions within the context of this mock trial. Please proceed with your inquiries.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# from openai import OpenAI\n",
    "# client = OpenAI()\n",
    "\n",
    "# completion = client.chat.completions.create(\n",
    "#     model=\"gpt-4o-mini\",  # Choose model with sufficient token capacity\n",
    "#     messages=[{\"role\": \"system\", \"content\": roleplay_prompt}]\n",
    "# )\n",
    "\n",
    "# print(completion.choices[0].message)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
